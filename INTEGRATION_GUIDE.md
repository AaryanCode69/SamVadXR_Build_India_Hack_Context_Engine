# Samvad XR â€” Integration Guide for Developer B

> **What this document is:** A clear contract between us. It tells you exactly what I need from your modules, what I'll do with them, and how our code connects at runtime.
>
> **TL;DR:** You build the ears (STT), the mouth (TTS), the cultural memory (RAG), and the conversation memory. I build the brain (LLM agent), the game rules (state engine), and the API that Unity talks to. I import your modules and call your functions in a specific order.

---

## 1. The Big Picture â€” Who Owns What

### Complete Request Lifecycle (Finalized)

```
Step  Owner   Module              Action                                         Time
â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€
 1    Dev A   main.py             Receive POST /api/interact, parse request        0ms
 2    Dev B   middleware.py        base64_to_bytes(request.audio_base64)           1ms
 3    Dev B   voice_ops.py         await transcribe_with_sarvam(bytes, "hi-IN")  ~800ms
                                   â†’ "à¤­à¤¾à¤ˆ à¤¯à¥‡ silk scarf à¤•à¤¿à¤¤à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ˆ?"
 4    Dev B   context_memory.py    memory.add_turn("user", text, metadata)         1ms
 5    Dev B   context_memory.py    context_block = memory.get_context_block()       1ms
 6    Dev B   rag_ops.py           rag_ctx = await retrieve_context(text, 3)      ~50ms
 7    Dev A   ai_brain.py          Compose prompt (context_block + rag_ctx         ~2s
                                     + Neo4j state) â†’ LLM â†’ parse JSON
 7Â½   Dev A   state_engine.py      Validate via Neo4j: clamp mood Â±15,            ~20ms
                                     verify stage transition is legal
 8    Dev B   context_memory.py    memory.add_turn("vendor", reply, metadata)       1ms
 9    Dev B   voice_ops.py         audio = await speak_with_sarvam(reply, "hi-IN") ~600ms
10    Dev B   middleware.py         b64 = bytes_to_base64(audio)                     1ms
11    Dev A   main.py              Return InteractResponse to Unity                 0ms
                                                                    TOTAL â‰ˆ 3.5s
```

**Your modules power 8 of the 11 steps.** But I'm the one calling them, in this exact sequence, inside my endpoint function.

> **Note on Neo4j (Steps 7 & 7Â½):** I use Neo4j as the persistent state store for session game state (mood, stage, turn count, price history). This is entirely my domain â€” you never interact with Neo4j directly. Your conversation memory (`context_memory.py`) handles dialogue history; my Neo4j graph handles game logic state.

---

## 2. Your Modules â€” What I Need From Each

### 2.1 `middleware.py` â€” Encoding Utilities

These are simple, synchronous helper functions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: base64_to_bytes(b64_string: str) -> bytes â”‚
â”‚                                                      â”‚
â”‚  Input:  "SGVsbG8gV29ybGQ="  (base64 string)        â”‚
â”‚  Output: b"Hello World"       (raw bytes)            â”‚
â”‚                                                      â”‚
â”‚  Called at: Step 2 (before STT)                      â”‚
â”‚  Error case: If input is invalid base64, raise       â”‚
â”‚              ValueError with a clear message         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: bytes_to_base64(audio_bytes: bytes) -> strâ”‚
â”‚                                                      â”‚
â”‚  Input:  b"\x00\x01\x02..."  (raw audio bytes)      â”‚
â”‚  Output: "AAEC..."            (base64 string)        â”‚
â”‚                                                      â”‚
â”‚  Called at: Step 10 (after TTS)                      â”‚
â”‚  Error case: Should never fail on valid bytes        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 `voice_ops.py` â€” Sarvam STT & TTS

These are the slowest steps in the pipeline. They MUST be `async`.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: transcribe_with_sarvam(                             â”‚
â”‚      audio_bytes: bytes,                                       â”‚
â”‚      language_code: str        # "hi-IN", "kn-IN", "ta-IN",   â”‚
â”‚  ) -> str                        "en-IN", "hi-EN"             â”‚
â”‚                                                                â”‚
â”‚  Input:  Raw audio bytes from the VR headset mic               â”‚
â”‚  Output: Transcribed text as a string                          â”‚
â”‚          e.g. "à¤­à¤¾à¤ˆ à¤¯à¥‡ silk scarf à¤•à¤¿à¤¤à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ˆ?"                    â”‚
â”‚                                                                â”‚
â”‚  Called at: Step 3                                              â”‚
â”‚  Latency budget: ~800ms (your estimate)                        â”‚
â”‚  My timeout: 5 seconds                                         â”‚
â”‚                                                                â”‚
â”‚  âš ï¸  QUESTIONS I NEED ANSWERED:                                â”‚
â”‚  1. What do you return if audio is silence/noise?              â”‚
â”‚     â†’ Empty string ""? Or do you raise an exception?           â”‚
â”‚  2. What exception type do you raise on Sarvam API failure?    â”‚
â”‚     â†’ I need to catch it specifically in my error handler      â”‚
â”‚  3. Does this function handle retries internally, or should I? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: speak_with_sarvam(                                  â”‚
â”‚      text: str,                                                â”‚
â”‚      language_code: str                                        â”‚
â”‚  ) -> bytes                                                    â”‚
â”‚                                                                â”‚
â”‚  Input:  Vendor's reply text                                   â”‚
â”‚          e.g. "à¤…à¤°à¥‡ à¤­à¤¾à¤ˆ, à¤¯à¥‡ pure silk à¤¹à¥ˆ! â‚¹800 à¤•à¤¾ à¤¹à¥ˆ"             â”‚
â”‚  Output: Audio bytes (WAV or MP3 â€” which format?)              â”‚
â”‚                                                                â”‚
â”‚  Called at: Step 9                                              â”‚
â”‚  Latency budget: ~600ms (your estimate)                        â”‚
â”‚  My timeout: 5 seconds                                         â”‚
â”‚                                                                â”‚
â”‚  âš ï¸  QUESTIONS I NEED ANSWERED:                                â”‚
â”‚  1. What audio format/encoding? WAV 16-bit PCM? MP3?          â”‚
â”‚     â†’ Unity needs to know what to decode on the other end      â”‚
â”‚  2. What sample rate? 16kHz? 22kHz? 44.1kHz?                  â”‚
â”‚  3. What happens if Sarvam TTS is down?                        â”‚
â”‚     â†’ I'll fall back to sending text-only (no audio),          â”‚
â”‚       but I need to know what exception to catch               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 `context_memory.py` â€” Conversation Memory

This tracks the full back-and-forth history within a session.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Class: ConversationMemory                                     â”‚
â”‚                                                                â”‚
â”‚  Method: add_turn(                                             â”‚
â”‚      role: str,           # "user" or "vendor"                 â”‚
â”‚      text: str,           # what was said                      â”‚
â”‚      metadata: dict       # extra info (see below)             â”‚
â”‚  ) -> None                                                     â”‚
â”‚                                                                â”‚
â”‚  I call this TWICE per request:                                â”‚
â”‚                                                                â”‚
â”‚  Call 1 (Step 4) â€” after STT:                                  â”‚
â”‚    memory.add_turn("user", "à¤­à¤¾à¤ˆ à¤¯à¥‡ silk scarf à¤•à¤¿à¤¤à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ˆ?", { â”‚
â”‚        "held_item": "silk_scarf",                              â”‚
â”‚        "looked_at_item": "brass_statue",                       â”‚
â”‚        "mood": 55,                                             â”‚
â”‚        "stage": "BROWSING"                                     â”‚
â”‚    })                                                          â”‚
â”‚                                                                â”‚
â”‚  Call 2 (Step 8) â€” after AI decides:                           â”‚
â”‚    memory.add_turn("vendor", "à¤…à¤°à¥‡ à¤­à¤¾à¤ˆ, à¤¯à¥‡ pure silk à¤¹à¥ˆ!...", { â”‚
â”‚        "mood": 60,                                             â”‚
â”‚        "stage": "HAGGLING",                                    â”‚
â”‚        "price": 700                                            â”‚
â”‚    })                                                          â”‚
â”‚                                                                â”‚
â”‚  âš ï¸  QUESTION: Do you need specific metadata keys, or is it   â”‚
â”‚  an arbitrary dict you store as-is?                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Method: get_context_block() -> str                            â”‚
â”‚                                                                â”‚
â”‚  Called at: Step 5 (before I build the LLM prompt)             â”‚
â”‚                                                                â”‚
â”‚  What I expect back: A formatted string of recent conversation â”‚
â”‚  history that I can inject directly into the LLM prompt.       â”‚
â”‚                                                                â”‚
â”‚  Example output:                                               â”‚
â”‚  """                                                           â”‚
â”‚  [Turn 1] User: Namaste bhaiya, kya haal hai?                 â”‚
â”‚  [Turn 1] Vendor: Aao aao! Kya chahiye aapko?                 â”‚
â”‚  [Turn 2] User: à¤­à¤¾à¤ˆ à¤¯à¥‡ silk scarf à¤•à¤¿à¤¤à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ˆ?                  â”‚
â”‚  """                                                           â”‚
â”‚                                                                â”‚
â”‚  âš ï¸  QUESTIONS:                                                â”‚
â”‚  1. How many turns does this include? Last 5? Last 10? All?    â”‚
â”‚     â†’ I'd suggest last 10 turns max to control token usage     â”‚
â”‚  2. Does the format include metadata (mood, price) or just     â”‚
â”‚     the spoken text?                                           â”‚
â”‚     â†’ I prefer text-only in the context block. I'll inject     â”‚
â”‚       current mood/stage separately from scene_context.        â”‚
â”‚  3. Can I configure the window size (number of turns)?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Important â€” Memory Instance Lifecycle:**

I will create and manage the `ConversationMemory` instance per session on my side:

```
# In my orchestration code (conceptual)
sessions = {}   # session_id â†’ ConversationMemory

def get_memory(session_id: str) -> ConversationMemory:
    if session_id not in sessions:
        sessions[session_id] = ConversationMemory()
    return sessions[session_id]
```

This means your `ConversationMemory` class should:
- Be instantiable with no required arguments (or with a `session_id` if you need it)
- Store state in the instance (not in a global/singleton)
- Be safe to create many instances (one per active VR session)

Let me know if you had a different design in mind (e.g., a singleton with internal session routing).

### 2.4 `rag_ops.py` â€” ChromaDB Retrieval

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: retrieve_context(                                   â”‚
â”‚      query: str,           # the user's transcribed text       â”‚
â”‚      n_results: int = 3    # how many chunks to return         â”‚
â”‚  ) -> str                                                      â”‚
â”‚                                                                â”‚
â”‚  Called at: Step 6                                              â”‚
â”‚  Latency budget: ~50ms (your estimate)                         â”‚
â”‚  My timeout: 3 seconds                                         â”‚
â”‚                                                                â”‚
â”‚  What I expect back: A single string with the relevant         â”‚
â”‚  cultural/item knowledge, ready to inject into the LLM prompt. â”‚
â”‚                                                                â”‚
â”‚  Example output:                                               â”‚
â”‚  """                                                           â”‚
â”‚  - Silk scarves from Varanasi are known for Banarasi weave     â”‚
â”‚  - Typical retail price range: â‚¹500-â‚¹1500                     â”‚
â”‚  - Vendors usually start 2x above their minimum price          â”‚
â”‚  """                                                           â”‚
â”‚                                                                â”‚
â”‚  âš ï¸  QUESTIONS:                                                â”‚
â”‚  1. Is the return type a single concatenated string, or a      â”‚
â”‚     list of strings? â†’ I'd prefer a single string I can        â”‚
â”‚     drop directly into the prompt.                             â”‚
â”‚  2. What do you return when there are no relevant results?     â”‚
â”‚     â†’ Empty string ""? Or "No context available"?              â”‚
â”‚  3. Is ChromaDB running in-process or as a separate service?   â”‚
â”‚     â†’ Affects deployment setup                                 â”‚
â”‚  4. Is this function async or sync?                            â”‚
â”‚     â†’ If sync, I'll wrap it in asyncio.to_thread()             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. The Critical Question: Async or Sync?

My FastAPI server runs on an async event loop. If your functions make blocking I/O calls (HTTP requests to Sarvam, ChromaDB queries), they **must** be either:

| Your Function | Makes Network Call? | Must Be Async? |
|--------------|-------------------|----------------|
| `base64_to_bytes` | No | No (pure computation) |
| `bytes_to_base64` | No | No (pure computation) |
| `transcribe_with_sarvam` | Yes (Sarvam API) | **Yes** |
| `speak_with_sarvam` | Yes (Sarvam API) | **Yes** |
| `retrieve_context` | Yes (ChromaDB) | **Yes, or I'll wrap it** |
| `memory.add_turn` | No (in-memory) | No |
| `memory.get_context_block` | No (in-memory) | No |

**Preferred:** Use `httpx.AsyncClient` or `aiohttp` for your Sarvam API calls instead of `requests`. If you're using `requests` (synchronous), let me know â€” I'll wrap your calls in `asyncio.to_thread()`, but it's less efficient.

---

## 4. Language Code Format â€” Let's Standardize

Your pipeline uses Sarvam's format. Let's go with yours:

| Language | Code We'll Both Use |
|----------|-------------------|
| Hindi | `hi-IN` |
| English | `en-IN` |
| Hinglish | `hi-EN` |
| Kannada | `kn-IN` |
| Tamil | `ta-IN` |

I'll update my Pydantic enums to match. This is now the single source of truth.

---

## 5. Error Contract â€” What Should Happen When Things Break

I need to know what exceptions your functions raise so I can handle them properly. Here's what I propose â€” please confirm or correct:

| Scenario | Your Function | What You Should Do | What I'll Do |
|----------|--------------|-------------------|-------------|
| Sarvam STT API is down | `transcribe_with_sarvam` | Raise a specific exception (e.g., `SarvamServiceError`) | Return 503 to Unity: "Voice recognition unavailable" |
| Audio is silence/noise | `transcribe_with_sarvam` | Return empty string `""` | Vendor says "Kuch bola aapne?" (Did you say something?) |
| STT returns garbage | `transcribe_with_sarvam` | Return whatever Sarvam returns (your best effort) | My AI brain will handle garbled input gracefully |
| Sarvam TTS API is down | `speak_with_sarvam` | Raise `SarvamServiceError` | Return text-only response (empty audio, subtitle only) |
| ChromaDB has no results | `retrieve_context` | Return empty string `""` | AI brain proceeds without cultural context (still works, just less grounded) |
| ChromaDB is unreachable | `retrieve_context` | Raise `RAGServiceError` | I skip RAG, continue without it (graceful degradation) |

**My ask:** Define two exception classes I can import:
```python
class SarvamServiceError(Exception): ...
class RAGServiceError(Exception): ...
```

Or tell me what you're already using and I'll catch those.

---

## 6. What I Do Between Your Steps (The Invisible Work)

Between steps 6 and 9, there's a lot happening on my side that's invisible to you but critical to the product:

### Step 7 â€” The AI Brain (~2s)

I take **everything your functions produced**, combine it with **game state from Neo4j**, and compose a prompt for GPT-4o:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SYSTEM PROMPT (written by me â€” the "God Prompt")           â•‘
â•‘  â€¢ Vendor persona (Ramesh, 55, from Jaipur)                 â•‘
â•‘  â€¢ Behavioral rules based on mood ranges                    â•‘
â•‘  â€¢ State transition rules (GREETINGâ†’BROWSINGâ†’HAGGLINGâ†’...)  â•‘
â•‘  â€¢ Strict JSON output schema                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CONVERSATION HISTORY â† from your get_context_block()       â•‘
â•‘  [Turn 1] User: Namaste bhaiya!                             â•‘
â•‘  [Turn 1] Vendor: Aao! Kya chahiye?                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CULTURAL CONTEXT â† from your retrieve_context()            â•‘
â•‘  â€¢ Silk scarves retail â‚¹500-â‚¹1500                           â•‘
â•‘  â€¢ Vendors start at 2x minimum                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  GAME STATE â† from Neo4j (my domain, not yours)             â•‘
â•‘  â€¢ current_mood: 55                                          â•‘
â•‘  â€¢ current_stage: "BROWSING"                                 â•‘
â•‘  â€¢ turn_count: 3                                             â•‘
â•‘  â€¢ price_history: [1500, 1200]                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  SCENE CONTEXT â† from Unity request metadata                â•‘
â•‘  â€¢ held_item: "silk_scarf"                                   â•‘
â•‘  â€¢ looked_at_item: "brass_statue"                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  USER MESSAGE â† from your transcribe_with_sarvam()          â•‘
â•‘  "à¤­à¤¾à¤ˆ à¤¯à¥‡ silk scarf à¤•à¤¿à¤¤à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ˆ?"                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â”‚
                            â–¼
                    GPT-4o responds with:
            {
              "reply_text": "à¤…à¤°à¥‡ à¤­à¤¾à¤ˆ, à¤¯à¥‡ pure silk à¤¹à¥ˆ!...",
              "new_mood": 60,
              "new_stage": "HAGGLING",
              "price_offered": 700,
              "internal_reasoning": "User is directly asking..."
            }
```

### Step 7Â½ â€” State Validation via Neo4j (~20ms)

I validate the AI's output against the state graph in Neo4j:
- Clamp mood to 0â€“100, max Â±15 change per turn
- Verify the stage transition is legal (e.g., can't jump from GREETING to DEAL)
- If the AI hallucinates an illegal state, I override it and keep the current state
- Write the validated new state back to Neo4j

This is fully my responsibility. You never interact with Neo4j.

---

## 7. What I'm Building While You're Building

So we're not blocked on each other, here's what I'm doing in parallel:

| My Task | Why You Don't Need to Wait |
|---------|---------------------------|
| Mocking all your functions | I have fake versions of `transcribe_with_sarvam`, `speak_with_sarvam`, `retrieve_context`, and `ConversationMemory` that return hardcoded data. I can test my full pipeline without your code. |
| Writing the GPT-4o system prompt | No dependency on you. Pure prompt engineering. |
| Building the state machine + Neo4j graph | No dependency on you. Pure game logic. Neo4j stores session state (mood, stage, turn count, price history). |
| Defining Pydantic models | I'll share the OpenAPI schema with you so you know exactly what the request/response looks like. |

**When you're ready**, I swap the mocks for your real implementations via a config toggle (`USE_MOCKS=true/false`). Zero code changes in my orchestration logic.

---

## 8. File Structure â€” Where Your Code Lives

```
SamVadXR-Orchestration/
â”‚
â”œâ”€â”€ app/                          â—„â”€â”€ MY domain
â”‚   â”œâ”€â”€ main.py                   # API endpoint, orchestration
â”‚   â”œâ”€â”€ models/                   # Pydantic request/response models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai_brain.py           # GPT-4o prompt + parsing
â”‚   â”‚   â”œâ”€â”€ state_engine.py       # State machine validation (Neo4j-backed)
â”‚   â”‚   â”œâ”€â”€ session_store.py      # Neo4j session state read/write
â”‚   â”‚   â””â”€â”€ mocks.py              # Mock versions of YOUR functions
â”‚   â”œâ”€â”€ prompts/                  # System prompt templates
â”‚   â””â”€â”€ config.py                 # Env vars, feature flags
â”‚
â”œâ”€â”€ services/                     â—„â”€â”€ YOUR domain
â”‚   â”œâ”€â”€ voice_ops.py              # transcribe_with_sarvam, speak_with_sarvam
â”‚   â”œâ”€â”€ rag_ops.py                # retrieve_context
â”‚   â”œâ”€â”€ context_memory.py         # ConversationMemory class
â”‚   â”œâ”€â”€ middleware.py             # base64_to_bytes, bytes_to_base64
â”‚   â””â”€â”€ exceptions.py            # SarvamServiceError, RAGServiceError
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_voice_ops.py         â—„â”€â”€ You write these
â”‚   â”œâ”€â”€ test_rag_ops.py           â—„â”€â”€ You write these
â”‚   â”œâ”€â”€ test_api.py               â—„â”€â”€ I write these (uses mocks or real)
â”‚   â””â”€â”€ test_integration.py       â—„â”€â”€ We write together
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

You can develop your `services/` folder independently. I import from it.

---

## 9. The Handshake Checklist

Before we integrate, let's confirm these decisions. Reply with your answers:

| # | Decision | Options | Your Answer |
|---|----------|---------|-------------|
| 1 | Are `transcribe_with_sarvam` and `speak_with_sarvam` async? | `async def` / `def` | ? |
| 2 | Is `retrieve_context` async? | `async def` / `def` | ? |
| 3 | What HTTP client do you use for Sarvam? | `httpx` / `aiohttp` / `requests` | ? |
| 4 | What does STT return on silence? | `""` / raises exception | ? |
| 5 | What audio format does TTS return? | WAV PCM / MP3 / OGG | ? |
| 6 | What sample rate for TTS audio? | 16kHz / 22kHz / 44.1kHz | ? |
| 7 | Does `retrieve_context` return `str` or `list[str]`? | `str` / `list[str]` | ? |
| 8 | Is `ConversationMemory` instance-based or singleton? | Instance per session / Singleton | ? |
| 9 | What exceptions do you raise for service failures? | Custom class name(s) | ? |
| 10 | Language code format confirmed? | `hi-IN` style | ? |
| 11 | `get_context_block()` â€” how many turns included? | Last N turns (what N?) | ? |
| 12 | Is ChromaDB in-process or a separate service? | In-process / External | ? |

---

## 10. Timeline & Integration Points

```
Week 1:
  You: Build voice_ops.py (STT/TTS with Sarvam)
  Me:  Build orchestration + AI brain + state engine (all mocked)
  
  âœ… Checkpoint: I send you a mock request/response JSON pair
     so you can verify your functions produce compatible shapes.

Week 2:
  You: Build rag_ops.py + context_memory.py
  Me:  Finish prompt tuning + state machine testing
  
  ðŸ¤ Integration Point: I import your modules, toggle USE_MOCKS=false
     We test the full pipeline together with a real audio clip.

Week 2-3:
  Together: End-to-end testing, latency profiling, edge case handling.
  Goal: Full loop under 4 seconds, all error cases produce graceful responses.
```

---

## Quick Reference â€” Function Signatures I'm Coding Against

```python
# middleware.py
def base64_to_bytes(b64_string: str) -> bytes: ...
def bytes_to_base64(audio_bytes: bytes) -> str: ...

# voice_ops.py
async def transcribe_with_sarvam(audio_bytes: bytes, language_code: str) -> str: ...
async def speak_with_sarvam(text: str, language_code: str) -> bytes: ...

# rag_ops.py
async def retrieve_context(query: str, n_results: int = 3) -> str: ...

# context_memory.py
class ConversationMemory:
    def add_turn(self, role: str, text: str, metadata: dict) -> None: ...
    def get_context_block(self) -> str: ...

# exceptions.py
class SarvamServiceError(Exception): ...
class RAGServiceError(Exception): ...
```

**These are the interfaces I'm mocking now and will swap for your real implementations later. If any signature doesn't work for you, let's discuss before either of us writes too much code.**

---

*Last updated: 2026-02-13 â€” Developer A*
